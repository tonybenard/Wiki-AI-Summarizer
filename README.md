# Wikipedia Summarizer with LLM  

This project scrapes **Wikipedia articles** and generates clear, concise summaries using **Large Language Models (LLMs)**.  
It combines **Selenium + BeautifulSoup** for scraping and **Ollama** for local LLM inference.    

---

##  Features
- ğŸ” Search Wikipedia directly from the script  
-  Clean extracted text (removes tables, images, nav, citations, etc.)  
-  Summarize with LLM (**Llama 3.2**)  
-  Outputs summaries in **bullet point format**  
- ğŸ› ï¸ Modular code (`main.py` + `scrap_site.py`)  

---

## ğŸ“‚ Project Structure
â”œâ”€â”€ main.py # Main entry point: scraping + summarization
â”œâ”€â”€ scrap_site.py # Wikipedia scraper class (Selenium + BeautifulSoup)
â””â”€â”€ README.md # Project documentation


---

## âš™ï¸ Installation

1. **Clone this repo**
   ```bash
   git clone https://github.com/tonybenard/Wiki-AI-Summarizer.git
   cd Wiki-AI-Summarizer

---

## ğŸ› ï¸ Tech Stack

Python 3.9+

Selenium â€“ browser automation

BeautifulSoup4 â€“ HTML parsing

Ollama â€“ LLM inference (using Llama 3.2 in this project)

ChromeDriver â€“ Selenium 

---

## ğŸ“Œ Notes

This script opens a Chrome browser window to interact with Wikipedia.

Make sure Ollama is installed and running before executing.

Summaries are generated locally, so no external API calls.

---

## Sample
<img src='wiki-ai-summarizer.png'>


